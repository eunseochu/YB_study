{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3UBWpG8GJt+cnC5/KjQvy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eunseochu/YB_study/blob/main/%ED%8C%8C%EC%9D%B4%EC%8D%AC_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D_%EC%99%84%EB%B2%BD_%EA%B0%80%EC%9D%B4%EB%93%9C_%EB%B6%84%EB%A5%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **01) 개요**"
      ],
      "metadata": {
        "id": "K4ntDaTgz0YU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **앙상블**\n",
        "결정 트리 기반의 다수의 약한 학습기(Weak Learner)를 결합해 변동성을 줄여 예측 오류를 줄이고 성능을 개선 <br/>\n",
        "분류를 위해 일반적으로 가장 많이 사용되며, 대표적으로 배깅과 부스팅으로 구분"
      ],
      "metadata": {
        "id": "KDTM55WAz9Y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **배깅** <br/>\n",
        "학습 데이터를 중복을 허용하면서 다수의 세트로 샘플링하여 이를 다수의 약한 학습기가 학습한 뒤, 최종 결과를 결합해 예측하는 방식\n",
        "- 랜덤 포레스트: 수행 시간이 빠르고 비교적 안정적인 예측 성능을 제공하는 훌륭한 머신러닝 알고리즘"
      ],
      "metadata": {
        "id": "DLOkXkEY0AOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **부스팅**\n",
        "학습기들이 순차적으로 학습을 진행하면서 예측이 틀린 데이터에 대해서는 가중치를 부여해 <br/>\n",
        "다음번 학습기가 학습할 때에는 이전에 예측이 틀린 데이터에 대해서는 보다 높은 정확도로 예측하는 방식\n",
        "- GBM: 뛰어난 예측 성능을 가졌지만, 수행 시간이 너무 오래 걸림\n",
        "- XGBoost: 많은 캐글 경연대회에서 우승을 위한 알고리즘으로 불리며 명성을 쌓아옴\n",
        "- LightGBM: XGBoost보다 빠른 학습 수행 시간 및 버금가는 예측 성능 보유\n",
        "- XGBoost & LightGBM은 사이킷런과의 쉬운 연동을 위해 사이킷런 래퍼 클래스 제공"
      ],
      "metadata": {
        "id": "EpfFyrg80JmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **스태킹 모델** \n",
        "여러 개의 개별 모델들이 생성한 예측 데이터를 기반으로 <br/>\n",
        "최종 메타 모델이 학습할 별도의 학습 데이터 세트와 예측할 테스트 데이터 세트를 재생성하는 기법"
      ],
      "metadata": {
        "id": "izhYnScJ0QIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **02) 결정트리**"
      ],
      "metadata": {
        "id": "NZtlz_-4mJ16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **결정트리란?** <br/>\n",
        "ML 알고리즘 중 직관적으로 이해하기 쉬운 알고리즘\n",
        "- 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만드는 것\n",
        "- 규칙 노드는 규칙 조건, 리프 노드는 결정된 클래스 값\n",
        "- 새로운 규칙 조건마다 서브 트리가 생성되는 방식"
      ],
      "metadata": {
        "id": "t-VCATRPn3xP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **결정트리 유의사항**\n",
        "규칙이 많으면 분류를 결정하는 방식이 더욱 복잡해지며, 과적합으로 이어지기 쉬움\n",
        "- 트리의 깊이가 깊어질수록 결정 트리의 예측 성능이 저하될 가능성이 높음\n",
        "- 가능한 적은 결정 노드로 높은 예측 정확도를 갖기 위해서는, <br/>\n",
        " 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 결정 노드의 규칙을 지정해야 함"
      ],
      "metadata": {
        "id": "TW7saiVLoG71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **결정트리의 특징**\n",
        "정보의 '균일도'라는 룰을 기반으로 하고 이어 알고리즘이 쉽고 직관적\n",
        "- 룰이 매우 명확하여 시각화로 표현할 수 있음\n",
        "- 특별한 경우를 제외하고는 각 피처의 스케일링과 정규화같은 전처리 작업이 필요없음\n",
        "- 과적합으로 정확도가 떨어진다는 단점존재"
      ],
      "metadata": {
        "id": "OjHTrxIsoZ_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **03) 앙상블 학습**"
      ],
      "metadata": {
        "id": "XSleGUauotNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **앙상블 학습이란?**\n",
        "여러 개의 분류기를 생성하여 그 예측을 결합함으로써 보다 정확한 최종 예측을 도출하는 기법\n",
        "- 다양한 분류기의 예측 결과를 결합함으로써 단일 분류기보다 신뢰성이 높은 예측값을 얻는 것\n",
        "- 정형 데이터 분류 시에는 앙상블이 뛰어난 성능을 나타내고 있음\n",
        "- 보팅(Voting), 배깅(Bagging), 부스팅(Boosting) 세 가지 유형으로 나눌 수 있음"
      ],
      "metadata": {
        "id": "udTKpOSwowSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **보팅(Voting)과 배깅(Bagging)**\n",
        "여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식\n",
        "- 보팅의 경우 일반적으로 서로 다른 알고리즘을 가진 분류기를 결합\n",
        "- 배깅의 경우 각각의 분류기가 모두 같은 유형의 알고리즘 기반이지만, <br/>\n",
        "데이터 샘플링을 서로 다르게 가져가며 학습을 수행해 보팅을 수행\n",
        " - 대표적인 배깅 방식: 랜덤 포레스트 알고리즘"
      ],
      "metadata": {
        "id": "hqS2uMKmpDHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **부스팅(Boosting)**\n",
        "여러 개의 분류기가 순차적으로 학습을 수행하되, 앞에서 학습한 분류기가 예측이 틀린 데이터에 대해서는 올바르게 예측할 수 있도록 <br/> 다음 분류기에는 가중치를 부여하며 학습과 예측 진행\n",
        "- 예측 성능이 뛰어나 앙상블 학습을 주도\n",
        "- 그래디언트 부스트, XGBoost, LightGBM 등 "
      ],
      "metadata": {
        "id": "yfjZS3z2qECl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **04) 랜덤 포레스트**"
      ],
      "metadata": {
        "id": "NtNbQ-C_qfJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **랜덤 포레스트란?**\n",
        "배깅의 대표적인 알고리즘으로, 다재 다능한 알고리즘\n",
        "- 비교적 빠른 수행 속도를 가지고 있으며, 다양한 영역에서 높은 예측 성능을 보이고 있음\n",
        "- 결정 트리를 기반 알고리즘으로 하고 있기에 결정 트리의 쉽고 직관적인 장점을 그대로 가지고 있음\n",
        "- 여러 개의 결정 트리 분류기가 전체 데이터에서 배깅 방식으로 각자의 데이터를 샘플링해 개별적으로 학습을 수행한 뒤, <br/> 최종적으로 모든 분류기가 보팅을 통해 예측 결정을 하게 됨"
      ],
      "metadata": {
        "id": "q2GN3v5Zqk6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **랜덤 포레스트 하이퍼 파라미터 및 튜닝**"
      ],
      "metadata": {
        "id": "Cw1Dqaj-rN8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "하이퍼 파라미터가 너무 많고, 그로 인해 튜닝을 위한 시간이 많이 소모됨\n",
        "- 많은 시간을 소모해 튜닝을 진행해도 예측 성능이 크게 향상되지 않음"
      ],
      "metadata": {
        "id": "R8Qar5mkrQyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **랜덤 포레스트 하이퍼 파라미터**\n",
        "- n_estimators: 결정 트리의 개수\n",
        " - 많이 설정할수록 좋은 성능을 기대할 수 있지만 계속 증가시킨다고 성능이 무조건 향상되는 것은 아님\n",
        " - 늘릴수록 학습 수행 시간이 오래 걸림\n",
        "- max_depth, min_sample_leaf: 결정 트리에서 과적합을 개선하기 위해 사용되는 파라미터"
      ],
      "metadata": {
        "id": "GR2614EWrboY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **GridSearchCV를 이용한 하이퍼파라미터 튜닝** <br/>\n",
        "현재 개정판 데이터 관련 오류가 있으나 해결 방법을 알 수 없음"
      ],
      "metadata": {
        "id": "vq4WibmIvA3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_new_feature_name_df(old_feature_name_df):\n",
        "    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(),\n",
        "                                  columns=['dup_cnt'])\n",
        "    feature_dup_df = feature_dup_df.reset_index()\n",
        "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')\n",
        "    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1]) \n",
        "                                                                                         if x[1] >0 else x[0] ,  axis=1)\n",
        "    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)\n",
        "    return new_feature_name_df\n",
        "\n",
        "def get_human_dataset( ):\n",
        "    \n",
        "    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.\n",
        "    feature_name_df = pd.read_csv('/content/features.txt',sep='\\s+',\n",
        "                        header=None,names=['column_index','column_name'])\n",
        "    \n",
        "    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame생성. \n",
        "    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n",
        "    \n",
        "    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환\n",
        "    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n",
        "    \n",
        "    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용\n",
        "    X_train = pd.read_csv('/content/X_train.txt',sep='\\s+', names=feature_name )\n",
        "    X_test = pd.read_csv('/content/X_test.txt',sep='\\s+', names=feature_name)\n",
        "    \n",
        "    # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여\n",
        "    y_train = pd.read_csv('/content/y_train.txt',sep='\\s+',header=None,names=['action'])\n",
        "    y_test = pd.read_csv('/content/y_test.txt',sep='\\s+',header=None,names=['action'])\n",
        "    \n",
        "    # 로드된 학습/테스트용 DataFrame을 모두 반환 \n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "5NzWrXUVvEKE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 결정 트리에서 사용한 get_human_dataset( )을 이용해 학습/테스트용 DataFrame 반환\n",
        "X_train, X_test, y_train, y_test = get_human_dataset()\n",
        "\n",
        "# 랜덤 포레스트 학습 및 별도의 테스트 셋으로 예측 성능 평가\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=0, max_depth=8)\n",
        "rf_clf.fit(X_train , y_train)\n",
        "pred = rf_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test , pred)\n",
        "print('랜덤 포레스트 정확도: {0:.4f}'.format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "lRfJi1bnvHVa",
        "outputId": "1d782410-edc3-490e-bc20-c839e87e7bac"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-140e8a3cd793>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 랜덤 포레스트 학습 및 별도의 테스트 셋으로 예측 성능 평가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mrf_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrf_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    112\u001b[0m         ):\n\u001b[1;32m    113\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"infinity\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NaN, infinity\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[1;32m    116\u001b[0m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {\n",
        "    'max_depth': [8, 16, 24],\n",
        "    'min_samples_leaf' : [1, 6, 12],\n",
        "    'min_samples_split' : [2, 8, 16]\n",
        "}\n",
        "# RandomForestClassifier 객체 생성 후 GridSearchCV 수행\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
        "grid_cv = GridSearchCV(rf_clf , param_grid=params , cv=2, n_jobs=-1 )\n",
        "grid_cv.fit(X_train , y_train)\n",
        "\n",
        "print('최적 하이퍼 파라미터:\\n', grid_cv.best_params_)\n",
        "print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "NwrKwoPEvI6e",
        "outputId": "12106014-98d6-4bce-a9e1-f259c7396062"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-1269e8fe9f34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mrf_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgrid_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_clf\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgrid_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'최적 하이퍼 파라미터:\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mrefit_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1285, 7352]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf1 = RandomForestClassifier(n_estimators=100,  min_samples_leaf=6, max_depth=16,\n",
        "                                 min_samples_split=2, random_state=0)\n",
        "rf_clf1.fit(X_train , y_train)\n",
        "pred = rf_clf1.predict(X_test)\n",
        "print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test , pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "f66G_AnVvXNR",
        "outputId": "9a7ee69d-1802-4afe-d077-0237a66c804c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-8c4437f9abb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m rf_clf1 = RandomForestClassifier(n_estimators=100,  min_samples_leaf=6, max_depth=16,\n\u001b[1;32m      2\u001b[0m                                  min_samples_split=2, random_state=0)\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrf_clf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_clf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'예측 정확도: {0:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    328\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    962\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    112\u001b[0m         ):\n\u001b[1;32m    113\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"infinity\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NaN, infinity\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[1;32m    116\u001b[0m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **05) GBM**"
      ],
      "metadata": {
        "id": "-WyKLz95vekK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **GBM이란?**\n",
        "에이다부스트와 유사하지만 가중치 업데이트를 경사 하강법을 이용\n",
        "- 에이다부스트: 오류 데이터에 가중치를 부여하며 부스팅을 수행하는 대표적인 알고리즘\n",
        "- 경사 하강법: 반복 수행을 통해 오류를 최소화할 수 있도록 가중치의 업데이트 값을 도출하는 기법"
      ],
      "metadata": {
        "id": "nFAfgNbovnT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **GBM의 특징**\n",
        "랜덤 포레스트보다 예측 성능이 뛰어난 경우가 많지만, 수행 시간이 오래 걸리고 하이퍼 파라미터 튜닝 노력이 더 필요\n",
        "- 병렬 처리를 지원하지 않아 대용량 데이터의 경우 학습에 매우 많은 시간이 필요\n",
        "- 빠른 수행 시간을 바탕으로 쉽게 예측 결과를 도출하는 랜덤 포레스트와 반대됨"
      ],
      "metadata": {
        "id": "umX2AXVIv__e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **GBM 하이퍼 파라미터 튜닝**\n",
        "- learning_rae: GBM이 하급을 진행할때마다 적용하는 학습률\n",
        " - Weak Learner가 순차적으로 오류 값을 보정해 나가는 데 적용하는 계수\n",
        " - 기본값은 0.1로, 0~1 사이의 값을 지정할 수 있음\n",
        " - 너무 작은 값 적용 시 업데이트 되는 값이 작아져 최소 오류 값을 찾아 예측 성능이 높아질 가능성이 높지만, <br/>\n",
        " 많은 weak learner는 순차적인 반복이 필요해 수행 시간이 오래 걸리고, 최소 오류 값을 찾지 못할 수도 있음\n",
        " - n_estimator와 상호 보완적으로 조합해 사용\n",
        "- n_estimator: weak learner의 개수\n",
        " - 개수가 많을수록 예측 성능이 일정 수준까지는 좋아질 수 있으나 수행 시간이 오래 걸림\n",
        "- subsample: weak learner가 학습에 사용하는 데이터의 샘플링 비율\n",
        " - 1은 전체 학습 데이터를 기반으로 학습한다는 의미이며, 과적합이 염려되는 경우 1보다 작은 값으로 설정"
      ],
      "metadata": {
        "id": "8AAY83mXwNYQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **06) XGBoost**"
      ],
      "metadata": {
        "id": "duXGIUGOw7gM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **XGBoost란?**\n",
        "트리 기반의 앙상블 학습에서 가장 각광받고 있는 알고리즘 중 하나\n",
        "\n",
        "- 뛰어난 예측 성능: 일반적으로 분류와 회귀 영역에서 뛰어난 예측 성능 발휘\n",
        "- GBM 대비 빠른 수행 시간: 병렬 수행 및 다양한 기능으로 빠른 수행 성능 보장\n",
        "- 과적합 규제 (Regulation): 과적합에 좀 더 강한 내구성"
      ],
      "metadata": {
        "id": "dogGcCJvxDDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **XGBoost 주요 부스터 파라미터**\n",
        "- eta: 0과 1 사이의 값을 지정하여 부스팅 스텝을 반복적으로 수행할 때 업데이트되는 학습률 값\n",
        "- num_boost_rounds: GBM의 n_estimators와 같은 파라미터\n",
        "- min_child_weight: 트리에서 추가적으로 가지를 나눌지를 결정하기 위해 필요한 데이터들의 weight 총합\n",
        " - 과적합을 조절하기 위해 사용\n",
        "- gamma: 트리의 리프 노드를 추가적으로 나눌지를 결정할 최소 손실 감소 값\n",
        "- max_depth: 높으면 특정 피처 조건에 특화되어 룰 조건이 만들어지므로 과적합 가능성이 높아짐\n",
        "- sub_sample: 트리가 커져서 과적합되는 것을 제어하기 위해 데이터를 샘플링하는 비율을 지정\n",
        "- colsample_bytree:트리 생성에 필요한 피처(칼럼)를 임의로 샘플링하는 데 사용\n",
        " - 과적합을 조정하는 데 적용\n",
        "- lambda: 피처 개수가 많을 경우 적용을 검토하며 값이 클수록 과적합 감소 효과 존재\n",
        "- alpha: 피처 개수가 많을 경우 적용을 검토하며 값이 클수록 과적합 감소 효과 존재\n",
        "- scales_pos_weight: 특정 값으로 치우친 비대칭 클래스로 구성된 데이터 세트의 균형을 유지하기 위한 파라미터"
      ],
      "metadata": {
        "id": "iaQUT9sfxOIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **XGBoost 조기중단 기능**\n",
        "n_estimators에 지정한 부스팅 반복 횟수에 도달하지 않더라도, <br/>\n",
        "예측 오류가 더 이상 개선되지 않으면 반복을 끝까지 수행하지 않고 중지해 수행 시간 개선 가능"
      ],
      "metadata": {
        "id": "wuNobCFKx-56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **07) LightGBM**"
      ],
      "metadata": {
        "id": "-ukt-r1jyLvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **LightGBM이란?**\n",
        "XGBoost와 함께 부스팅 계열 알고리즘에서 가장 각광 받고 있음\n",
        "- 더 빠른 학습과 예측 수행 시간\n",
        "- 더 작은 메모리 사용량\n",
        "- 대용량 데이터에 대한 뛰어난 예측 성능\n",
        "- 병렬 컴퓨팅, GPU 등 다양한 기능 지원\n",
        "- 카테고리형 피처의 자동 변환과 최적 분할\n",
        "- 적은 데이터 세트에 적용할 경우 과적합이 발생하기 쉬움 (일반적으로 10,000건 이하)"
      ],
      "metadata": {
        "id": "Bd_mF7eHyQoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **XGBoost의 단점**\n",
        "Xgboost는 매우 뛰어난 부스팅 알고리즘이지만 학습 시간이 오래걸리며, <br/>\n",
        "GridSearchCV로 하이퍼 파라미터 튜닝을 수행 시 시간이 너무 오래 걸려 많은 파라미터를 튜닝하기 어려움"
      ],
      "metadata": {
        "id": "Sav0VNJPyb1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **LightGBM 하이퍼 파라미터**\n",
        "- num_iterations: 반복 수행하려는 트리의 개수 지정\n",
        " - 너무 크게 지정하면 오히려 과적합로 성능이 저하\n",
        "- learning_rate: 0에서 1 사이의 값을 지정하며 부스팅 스텝을 반복적으로 수행할 때 업데이트되는 학습률 값\n",
        " - 일반적으로 n_estimator를 크게 하고 learning_rate를 작게 해서 예측 성능을 향상\n",
        "- max_depth: 0보다 작은 값을 지정하면 깊이의 제한이 없음\n",
        "- min_data_in_leaf: 최종 결정 클래스인 리프 노드가 되기 위해서 최소한으로 필요한 레코드 수\n",
        " - 과적합을 제어하기 위한 파라미터\n",
        "- num_leaves: 하나의 트리가 가질 수 있는 최대 리프 개수\n",
        "- bagging_fraction: 트리가 커져서 과적합되는 것을 제어하기 위해서 데이터를 샘플링하는 비율을 지정\n",
        "- feature_fraction: 개별 트리를 학습할 때마다 무작위로 선택하는 피처의 비율\n",
        " - 과적합을 막기 위해 사용\n",
        "- lambda_l2: 피처 개수가 많을 경우 적용을 검토하며 값이 클수록 과적합 감소 효과\n",
        "- lambda_l1: L2와 마찬가지로 과적합 제어"
      ],
      "metadata": {
        "id": "sjKjpx-PykGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10) 스태킹 앙상블**"
      ],
      "metadata": {
        "id": "kcepXh8CzQHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **스태킹 앙상블이란?**\n",
        "개별적인 여러 알고리즘을 서로 결합해 예측 결과를 도출한다는 점에서 배깅 및 부스팅과 동일하지만, <br/>\n",
        "개별 알고리즘으로 예측한 데이터를 기반으로 다시 예측을 수행\n",
        "- 개별 알고리즘의 예측 결과 데이터 세트를 최종적인 메타 데이터 세트로 만들어, <br/>\n",
        "별도의 ML 알고리즘으로 최종 학습을 수행하고 테스트 데이터를 기반으로 다시 최종 예측 수행\n",
        "- 메타 모델: 개별 모델의 예측된 데이터 세트를 다시 기반으로 하여 학습하고 예측하는 방식"
      ],
      "metadata": {
        "id": "9u-tmKRczXl-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **스태킹 모델 활용** <br/>\n",
        "현실 모델에 적용하는 경우는 많지 않으며, 캐글과 같은 대회에서 높은 순위를 차지하기 위해 성능 수치를 높여야 할 경우 사용\n",
        "- 2~3개의 개별 모델만을 결합해서는 쉽게 예측 성능을 향상시킬 수 없으며, 스태킹을 적용한다고 반드시 성능 향상이 되는 것은 아님\n",
        "- 일반적으로 성능이 비슷한 모델을 결합해 좀 더 나은 성능 향상을 도출하기 위해 적용"
      ],
      "metadata": {
        "id": "_H2vSASozl8O"
      }
    }
  ]
}